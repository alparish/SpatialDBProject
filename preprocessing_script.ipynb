{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import glob\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add Trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trip_ids(df):\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    # Calculate time difference between consecutive rows\n",
    "    time_diff = df['datetime'].diff()\n",
    "    \n",
    "    # New trip starts when time difference > 30 minutes (and for first record)\n",
    "    new_trip = (time_diff > timedelta(minutes=30)) | (time_diff.isna())\n",
    "    \n",
    "    \n",
    "    df['trip_id'] = new_trip.cumsum()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_taxi_file(file_path):\n",
    "\n",
    "    df = pd.read_csv(file_path, \n",
    "                     names=['taxi_id', 'datetime', 'longitude', 'latitude'])\n",
    "    \n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    dropped_rows = initial_rows - len(df)\n",
    "    \n",
    "    if dropped_rows > 0:\n",
    "        print(f\"Removed {dropped_rows} duplicate rows from {file_path}\")\n",
    "        \n",
    " \n",
    "    df_with_trips = add_trip_ids(df)\n",
    "    \n",
    "    return df_with_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 24 duplicate rows from C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\Trajectory_Dataset\\1.txt\n",
      "Removed 219 duplicate rows from C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\Trajectory_Dataset\\10.txt\n",
      "Removed 51 duplicate rows from C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\Trajectory_Dataset\\100.txt\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame()\n",
    "\n",
    "for file_path in glob.glob('C:\\\\priya_hari\\\\UW Tacoma\\\\MSCSS\\\\Fourth Quarter\\\\TCSS_565\\\\Trajectory_Dataset\\\\*.txt'):\n",
    "    df = process_taxi_file(file_path)\n",
    "    result_df = pd.concat([result_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('processed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxi_id               int64\n",
       "datetime     datetime64[ns]\n",
       "longitude           float64\n",
       "latitude            float64\n",
       "trip_id               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxi_id      0\n",
       "datetime     0\n",
       "longitude    0\n",
       "latitude     0\n",
       "trip_id      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = result_df.isnull().sum()\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Connection and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_taxi_data(df, server, database):\n",
    "    conn_str = (\n",
    "        'Driver={ODBC Driver 18 for SQL Server};'\n",
    "        f'Server={server};'\n",
    "        f'Database={database};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        'TrustServerCertificate=yes;'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Create connection\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to the database\")\n",
    "        \n",
    "        rows_processed = 0\n",
    "        \n",
    "        # SQL insert statement using geography::Point\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO taxi_trips \n",
    "            (taxi_id, trip_id, datetime, longitude, latitude, location)\n",
    "        VALUES \n",
    "            (?, ?, ?, ?, ?, geography::Point(?, ?, 4326))\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\n",
    "                    insert_query,\n",
    "                    int(row['taxi_id']),\n",
    "                    int(row['trip_id']),\n",
    "                    row['datetime'],\n",
    "                    float(row['longitude']),\n",
    "                    float(row['latitude']),\n",
    "                    float(row['latitude']),    # Point takes latitude first\n",
    "                    float(row['longitude'])     # then longitude\n",
    "                )\n",
    "                \n",
    "                rows_processed += 1\n",
    "                if rows_processed % 100 == 0:  # Print progress every 100 rows\n",
    "                    print(f\"Processed {rows_processed} rows\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting row {row['taxi_id']}, {row['trip_id']}, {row['datetime']}: {str(e)}\")\n",
    "                raise\n",
    "                \n",
    "        # Commit the transaction and close connections\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(f\"Data insertion completed successfully. Total rows processed: {rows_processed}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to the database\n",
      "Processed 100 rows\n",
      "Processed 200 rows\n",
      "Processed 300 rows\n",
      "Processed 400 rows\n",
      "Processed 500 rows\n",
      "Processed 600 rows\n",
      "Processed 700 rows\n",
      "Processed 800 rows\n",
      "Processed 900 rows\n",
      "Processed 1000 rows\n",
      "Processed 1100 rows\n",
      "Processed 1200 rows\n",
      "Processed 1300 rows\n",
      "Processed 1400 rows\n",
      "Processed 1500 rows\n",
      "Processed 1600 rows\n",
      "Processed 1700 rows\n",
      "Processed 1800 rows\n",
      "Processed 1900 rows\n",
      "Processed 2000 rows\n",
      "Processed 2100 rows\n",
      "Processed 2200 rows\n",
      "Processed 2300 rows\n",
      "Processed 2400 rows\n",
      "Processed 2500 rows\n",
      "Processed 2600 rows\n",
      "Processed 2700 rows\n",
      "Processed 2800 rows\n",
      "Processed 2900 rows\n",
      "Processed 3000 rows\n",
      "Processed 3100 rows\n",
      "Processed 3200 rows\n",
      "Processed 3300 rows\n",
      "Processed 3400 rows\n",
      "Processed 3500 rows\n",
      "Processed 3600 rows\n",
      "Processed 3700 rows\n",
      "Processed 3800 rows\n",
      "Processed 3900 rows\n",
      "Processed 4000 rows\n",
      "Processed 4100 rows\n",
      "Processed 4200 rows\n",
      "Processed 4300 rows\n",
      "Processed 4400 rows\n",
      "Processed 4500 rows\n",
      "Processed 4600 rows\n",
      "Processed 4700 rows\n",
      "Processed 4800 rows\n",
      "Processed 4900 rows\n",
      "Processed 5000 rows\n",
      "Processed 5100 rows\n",
      "Processed 5200 rows\n",
      "Processed 5300 rows\n",
      "Processed 5400 rows\n",
      "Processed 5500 rows\n",
      "Processed 5600 rows\n",
      "Processed 5700 rows\n",
      "Processed 5800 rows\n",
      "Processed 5900 rows\n",
      "Processed 6000 rows\n",
      "Processed 6100 rows\n",
      "Processed 6200 rows\n",
      "Processed 6300 rows\n",
      "Processed 6400 rows\n",
      "Processed 6500 rows\n",
      "Processed 6600 rows\n",
      "Processed 6700 rows\n",
      "Processed 6800 rows\n",
      "Processed 6900 rows\n",
      "Processed 7000 rows\n",
      "Processed 7100 rows\n",
      "Processed 7200 rows\n",
      "Data insertion completed successfully. Total rows processed: 7234\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    server = 'RIYA_SURFACE\\\\PRIYAMSSQL'\n",
    "    database = 'TCSS565_TrajectoryDB'\n",
    "    \n",
    "        \n",
    "        # Then try the insert\n",
    "insert_taxi_data(result_df, server, database)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
