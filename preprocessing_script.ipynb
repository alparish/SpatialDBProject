{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import glob\n",
    "import pyodbc\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add Trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(point1, point2):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "\n",
    "    lat1, lon1 = radians(point1.y), radians(point1.x)\n",
    "    lat2, lon2 = radians(point2.y), radians(point2.x)\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def calculate_speed(row1, row2):\n",
    "    point1 = Point(row1['longitude'], row1['latitude'])\n",
    "    point2 = Point(row2['longitude'], row2['latitude'])\n",
    "    \n",
    "    # Calculate distance in kilometers\n",
    "    distance = haversine_distance(point1, point2)\n",
    "    \n",
    "    # Calculate time difference in hours\n",
    "    time_diff = (row2['datetime'] - row1['datetime']).total_seconds() / 3600\n",
    "    \n",
    "    # Calculate speed in km/h\n",
    "    if time_diff > 0:\n",
    "        speed = distance / time_diff\n",
    "    else:\n",
    "        speed = float('inf')\n",
    "    \n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_taxi_file(file_path, max_speed_kmh=120, min_points=2, max_points=15):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, \n",
    "                     names=['taxi_id', 'datetime', 'longitude', 'latitude'])\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {initial_rows - len(df)} duplicate rows\")\n",
    "    \n",
    "    # Convert datetime\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Sort by datetime\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    valid_indices = []\n",
    "    trip_ids = []\n",
    "    current_trip = []\n",
    "    current_trip_id = 0\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if not current_trip:  # If starting a new trip\n",
    "            current_trip.append(i)\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # Calculate speed from previous point\n",
    "        speed = calculate_speed(\n",
    "            df.iloc[current_trip[-1]], \n",
    "            df.iloc[i]\n",
    "        )\n",
    "        \n",
    "        # Check if time gap is too large (30 minutes)\n",
    "        time_gap = (df.iloc[i]['datetime'] - \n",
    "                   df.iloc[current_trip[-1]]['datetime']).total_seconds() / 60\n",
    "        \n",
    "        if time_gap > 30:  # End current trip and start new one\n",
    "            if len(current_trip) >= min_points:\n",
    "                valid_indices.extend(current_trip)\n",
    "                trip_ids.extend([current_trip_id] * len(current_trip))\n",
    "                current_trip_id += 1\n",
    "            current_trip = [i]\n",
    "            \n",
    "        elif len(current_trip) >= max_points:  # Trip reached max length\n",
    "            valid_indices.extend(current_trip)\n",
    "            trip_ids.extend([current_trip_id] * len(current_trip))\n",
    "            current_trip_id += 1\n",
    "            current_trip = [i]\n",
    "            \n",
    "        elif speed > max_speed_kmh:  # Skip this point if speed is too high\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        else:  # Add point to current trip\n",
    "            current_trip.append(i)\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    if len(current_trip) >= min_points:\n",
    "        valid_indices.extend(current_trip)\n",
    "        trip_ids.extend([current_trip_id] * len(current_trip))\n",
    "    \n",
    "    # Create filtered dataframe\n",
    "    if valid_indices:\n",
    "        df_filtered = df.iloc[valid_indices].copy()\n",
    "        df_filtered['trip_id'] = trip_ids\n",
    "        \n",
    "        return df_filtered\n",
    "    else:\n",
    "        return pd.DataFrame() \n",
    "\n",
    "def process_all_files(directory_path, max_speed_kmh=120):\n",
    "    result_df = pd.DataFrame()\n",
    "    file_pattern = f\"{directory_path}/*.txt\"\n",
    "    \n",
    "    total_files = 0\n",
    "    total_initial_rows = 0\n",
    "    total_final_rows = 0\n",
    "    total_trips = 0\n",
    "    \n",
    "    for file_path in glob.glob(file_pattern):\n",
    "        df_initial = pd.read_csv(file_path, \n",
    "                               names=['taxi_id', 'datetime', 'longitude', 'latitude'])\n",
    "        total_initial_rows += len(df_initial)\n",
    "        \n",
    "        df = process_taxi_file(file_path, max_speed_kmh=max_speed_kmh)\n",
    "        if not df.empty:\n",
    "            total_final_rows += len(df)\n",
    "            total_trips += df['trip_id'].nunique()\n",
    "            result_df = pd.concat([result_df, df], ignore_index=True)\n",
    "        \n",
    "        total_files += 1\n",
    "        print(f\"Processed {file_path}\")\n",
    "    \n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Total initial rows: {total_initial_rows}\")\n",
    "    print(f\"Total final rows: {total_final_rows}\")\n",
    "    print(f\"Total trips: {total_trips}\")\n",
    "    print(f\"Points removed: {total_initial_rows - total_final_rows}\")\n",
    "    print(f\"Average points per trip: {total_final_rows / total_trips:.2f}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the file path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 24 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\1.txt\n",
      "Removed 219 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\10.txt\n",
      "Removed 51 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\100.txt\n",
      "Removed 63 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\11.txt\n",
      "Removed 43 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\12.txt\n",
      "Removed 11 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\13.txt\n",
      "Removed 81 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\14.txt\n",
      "Removed 73 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\15.txt\n",
      "Removed 178 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\16.txt\n",
      "Removed 107 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\17.txt\n",
      "Removed 63 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\18.txt\n",
      "Removed 36 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\19.txt\n",
      "Removed 102 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\2.txt\n",
      "Removed 54 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\20.txt\n",
      "Removed 57 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\3.txt\n",
      "Removed 25 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\4.txt\n",
      "Removed 34 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\5.txt\n",
      "Removed 36 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\6.txt\n",
      "Removed 20 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\7.txt\n",
      "Removed 0 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\8.txt\n",
      "Removed 26 duplicate rows\n",
      "Processed C:\\priya_hari\\UW Tacoma\\MSCSS\\Fourth Quarter\\TCSS_565\\SpatialDBProject\\Trajectory_Dataset\\9.txt\n",
      "\n",
      "Processing Summary:\n",
      "Total files processed: 21\n",
      "Total initial rows: 27999\n",
      "Total final rows: 26192\n",
      "Total trips: 1901\n",
      "Points removed: 1807\n",
      "Average points per trip: 13.78\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"C:\\\\priya_hari\\\\UW Tacoma\\\\MSCSS\\\\Fourth Quarter\\\\TCSS_565\\\\SpatialDBProject\\\\Trajectory_Dataset\\\\\"\n",
    "result_df = process_all_files(directory_path, max_speed_kmh=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>trip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-02 15:36:08</td>\n",
       "      <td>116.51172</td>\n",
       "      <td>39.92123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-02 15:46:08</td>\n",
       "      <td>116.51135</td>\n",
       "      <td>39.93883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-02 15:56:08</td>\n",
       "      <td>116.51627</td>\n",
       "      <td>39.91034</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-02 16:06:08</td>\n",
       "      <td>116.47186</td>\n",
       "      <td>39.91248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-02 16:16:08</td>\n",
       "      <td>116.47217</td>\n",
       "      <td>39.92498</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26187</th>\n",
       "      <td>9</td>\n",
       "      <td>2008-02-08 12:23:56</td>\n",
       "      <td>117.10815</td>\n",
       "      <td>40.19025</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26188</th>\n",
       "      <td>9</td>\n",
       "      <td>2008-02-08 17:20:02</td>\n",
       "      <td>117.10815</td>\n",
       "      <td>40.19025</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26189</th>\n",
       "      <td>9</td>\n",
       "      <td>2008-02-08 17:26:02</td>\n",
       "      <td>117.10472</td>\n",
       "      <td>40.16563</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26190</th>\n",
       "      <td>9</td>\n",
       "      <td>2008-02-08 17:32:02</td>\n",
       "      <td>117.10505</td>\n",
       "      <td>40.13928</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26191</th>\n",
       "      <td>9</td>\n",
       "      <td>2008-02-08 17:38:02</td>\n",
       "      <td>117.10752</td>\n",
       "      <td>40.11415</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26192 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       taxi_id            datetime  longitude  latitude  trip_id\n",
       "0            1 2008-02-02 15:36:08  116.51172  39.92123        0\n",
       "1            1 2008-02-02 15:46:08  116.51135  39.93883        0\n",
       "2            1 2008-02-02 15:56:08  116.51627  39.91034        0\n",
       "3            1 2008-02-02 16:06:08  116.47186  39.91248        0\n",
       "4            1 2008-02-02 16:16:08  116.47217  39.92498        0\n",
       "...        ...                 ...        ...       ...      ...\n",
       "26187        9 2008-02-08 12:23:56  117.10815  40.19025       49\n",
       "26188        9 2008-02-08 17:20:02  117.10815  40.19025       50\n",
       "26189        9 2008-02-08 17:26:02  117.10472  40.16563       50\n",
       "26190        9 2008-02-08 17:32:02  117.10505  40.13928       50\n",
       "26191        9 2008-02-08 17:38:02  117.10752  40.11415       50\n",
       "\n",
       "[26192 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('processed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxi_id               int64\n",
       "datetime     datetime64[ns]\n",
       "longitude           float64\n",
       "latitude            float64\n",
       "trip_id               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------END-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = result_df.isnull().sum()\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Connection and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_taxi_data(df, server, database):\n",
    "    conn_str = (\n",
    "        'Driver={ODBC Driver 18 for SQL Server};'\n",
    "        f'Server={server};'\n",
    "        f'Database={database};'\n",
    "        'Trusted_Connection=yes;'\n",
    "        'TrustServerCertificate=yes;'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Create connection\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        cursor = conn.cursor()\n",
    "        print(\"Connected to the database\")\n",
    "        \n",
    "        rows_processed = 0\n",
    "        \n",
    "        # SQL insert statement using geography::Point\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO taxi_trips \n",
    "            (taxi_id, trip_id, datetime, longitude, latitude, location)\n",
    "        VALUES \n",
    "            (?, ?, ?, ?, ?, geography::Point(?, ?, 4326))\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\n",
    "                    insert_query,\n",
    "                    int(row['taxi_id']),\n",
    "                    int(row['trip_id']),\n",
    "                    row['datetime'],\n",
    "                    float(row['longitude']),\n",
    "                    float(row['latitude']),\n",
    "                    float(row['latitude']),    # Point takes latitude first\n",
    "                    float(row['longitude'])     # then longitude\n",
    "                )\n",
    "                \n",
    "                rows_processed += 1\n",
    "                if rows_processed % 100 == 0:  # Print progress every 100 rows\n",
    "                    print(f\"Processed {rows_processed} rows\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting row {row['taxi_id']}, {row['trip_id']}, {row['datetime']}: {str(e)}\")\n",
    "                raise\n",
    "                \n",
    "        # Commit the transaction and close connections\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(f\"Data insertion completed successfully. Total rows processed: {rows_processed}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    server = 'RIYA_SURFACE\\\\PRIYAMSSQL'\n",
    "    database = 'TCSS565_TrajectoryDB'\n",
    "    \n",
    "        \n",
    "insert_taxi_data(result_df, server, database)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
